{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for placing the hashed trajectories into buckets\n",
    "\n",
    "Utilizes the hashes of trajectories in Porto. \n",
    "\n",
    "Creates buckets for both Grid and disk scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root found: c:\\Users\\eivin\\dev\\JoonEndreLSH\\masteroppgave\n"
     ]
    }
   ],
   "source": [
    "# Importing nescessary modules\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import timeit as ti\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def find_project_root(target_folder=\"masteroppgave\"):\n",
    "    \"\"\"Find the absolute path of a folder by searching upward.\"\"\"\n",
    "    currentdir = os.path.abspath(\"__file__\")  # Get absolute script path\n",
    "    while True:\n",
    "        if os.path.basename(currentdir) == target_folder:\n",
    "            return currentdir  # Found the target folder\n",
    "        parentdir = os.path.dirname(currentdir)\n",
    "        if parentdir == currentdir:  # Stop at filesystem root\n",
    "            return None\n",
    "        currentdir = parentdir  # Move one level up\n",
    "\n",
    "# Example usage\n",
    "project_root = find_project_root(\"masteroppgave\")\n",
    "\n",
    "if project_root:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Project root found: {project_root}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find 'masteroppgave' directory\")\n",
    "\n",
    "from utils.helpers.save_trajectory import save_trajectory_hashes\n",
    "from utils.helpers import file_handler as fh\n",
    "from utils.helpers import metafile_handler as mfh\n",
    "from schemes.lsh_disk import DiskLSH\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read from json file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../hash_generation/porto_lsh_grid_current_parameters.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 6\u001b[0m     rome_lsh_grid_current_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../hash_generation/porto_lsh_disk_current_parameters.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m     rome_lsh_disk_current_parameters \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "porto_lsh_grid_current_parameters = {}\n",
    "porto_lsh_disk_current_parameters = {}\n",
    "\n",
    "# Read from json file\n",
    "with open(\"../hash_generation/porto_lsh_grid_current_parameters.json\", \"r\") as f:\n",
    "    porto_lsh_grid_current_parameters = json.load(f)\n",
    "    \n",
    "with open(\"../hash_generation/porto_lsh_disk_current_parameters.json\", \"r\") as f:\n",
    "    porto_lsh_disk_current_parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CityHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest Bucket: 210474472678124806769105046041730943051\n",
      "Total Buckets: 496\n",
      "Buckets with more than one trajectory: 4\n",
      "Buckets with only one trajectory: 492\n",
      "Largest Bucket Size: 2\n",
      "Percentage of buckets with more than one trajectory: 0.81%\n",
      "Percentage of buckets with only one trajectory: 99.19%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cityhash\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "PORTO_HASHED_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/hashed_data/grid/porto/\"\n",
    "PORTO_HASHED_TRAJECTORIES_FOLDER_META_FILE = f\"{PORTO_HASHED_TRAJECTORIES_OUTPUT_FOLDER}META-{porto_lsh_grid_current_parameters[\"number_of_trajectories\"]}.txt\"\n",
    "\n",
    "PORTO_FULL_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/porto/output/\"\n",
    "\n",
    "# Dictionary for the bucket system\n",
    "bucket_system = {}\n",
    "\n",
    "# Get filenames from the metafile\n",
    "files = mfh.read_meta_file(PORTO_HASHED_TRAJECTORIES_FOLDER_META_FILE)\n",
    "\n",
    "# Iterate through trajectory files and read their hashes\n",
    "for filename in files:\n",
    "    file_path = os.path.join(PORTO_HASHED_TRAJECTORIES_OUTPUT_FOLDER, filename)\n",
    "    \n",
    "    # Read the hashes for the trajectory\n",
    "    trajectory_hashes = fh.read_hash_file(file_path)\n",
    "    \n",
    "    # Iterate over each layer's hash\n",
    "    for layer_hash in trajectory_hashes:\n",
    "        # Convert the list of coordinates into a string\n",
    "        hash_string = \"_\".join(map(str, layer_hash))\n",
    "        \n",
    "        # Use CityHash for creating a unique key\n",
    "        hash_key = cityhash.CityHash128(hash_string)\n",
    "        \n",
    "        # Place trajectory into the appropriate bucket\n",
    "        if hash_key not in bucket_system:\n",
    "            bucket_system[hash_key] = []\n",
    "        bucket_system[hash_key].append(filename)\n",
    "\n",
    "\n",
    "\n",
    "# Write the bucket system to a JSON file\n",
    "output_file_path = f\"../../results_hashed/buckets/grid/porto/porto_grid_resolution_{porto_lsh_grid_current_parameters[\"resolution\"]}_layers_{porto_lsh_grid_current_parameters[\"layers\"]}_trajectories_{porto_lsh_grid_current_parameters[\"number_of_trajectories\"]}.json\"\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    json.dump(bucket_system, f)\n",
    "\n",
    "# Analyze and display results\n",
    "total_buckets = len(bucket_system)\n",
    "buckets_with_multiple = sum(1 for trajectories in bucket_system.values() if len(trajectories) > 1)\n",
    "buckets_with_single = total_buckets - buckets_with_multiple\n",
    "largest_bucket_size = max(len(trajectories) for trajectories in bucket_system.values())\n",
    "largest_bucket = max(bucket_system, key=lambda key: len(bucket_system[key]))\n",
    "print(f\"Largest Bucket: {largest_bucket}\")\n",
    "\n",
    "print(f\"Total Buckets: {total_buckets}\")\n",
    "print(f\"Buckets with more than one trajectory: {buckets_with_multiple}\")\n",
    "print(f\"Buckets with only one trajectory: {buckets_with_single}\")\n",
    "print(f\"Largest Bucket Size: {largest_bucket_size}\")\n",
    "\n",
    "# Optional: Display distribution percentages\n",
    "multiple_bucket_percentage = (buckets_with_multiple / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "single_bucket_percentage = (buckets_with_single / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "\n",
    "print(f\"Percentage of buckets with more than one trajectory: {multiple_bucket_percentage:.2f}%\")\n",
    "print(f\"Percentage of buckets with only one trajectory: {single_bucket_percentage:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finds the largest bucket and outputs stats from this bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket with the most trajectories: 210474472678124806769105046041730943051 (2 trajectories)\n",
      "Most Similar Pair (Lowest similarity value): 0.1406260090323557 between R_DUB.txt and R_ECP.txt\n",
      "Least Similar Pair (Highest similarity value): 0.1406260090323557 between R_DUB.txt and R_ECP.txt\n",
      "Average Similarity inside bucket '210474472678124806769105046041730943051': 0.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "PORTO_TRUE_SIMILARITY_FILE = \"../../results_true/similarity_values/porto/dtw/porto-dtw-3050.csv\"\n",
    "\n",
    "# Load the similarity matrix CSV\n",
    "similarity_df = pd.read_csv(PORTO_TRUE_SIMILARITY_FILE, index_col=0)\n",
    "\n",
    "# Results storage for statistics\n",
    "bucket_stats = {}\n",
    "\n",
    "# Filter buckets to process only those with more than one trajectory\n",
    "filtered_buckets = {bucket: trajectories for bucket, trajectories in bucket_system.items() if len(trajectories) > 1}\n",
    "\n",
    "# Track the bucket with the most trajectories\n",
    "max_bucket_size = 0\n",
    "max_bucket = None\n",
    "\n",
    "# Process each filtered bucket\n",
    "for bucket, trajectories in filtered_buckets.items():\n",
    "    similarities = []\n",
    "    best_pair = None\n",
    "    worst_pair = None\n",
    "\n",
    "    # Compute all pairwise similarities within the bucket\n",
    "    for t1, t2 in combinations(trajectories, 2):\n",
    "        # Strip `.txt` from the trajectory names\n",
    "        t1_clean = t1.replace('.txt', '')\n",
    "        t2_clean = t2.replace('.txt', '')\n",
    "\n",
    "        # Ensure correct row-column order (t1_clean should be lexicographically larger)\n",
    "        if t1_clean < t2_clean:\n",
    "            t1_clean, t2_clean = t2_clean, t1_clean  # Swap for correct matrix access\n",
    "\n",
    "        if t1_clean in similarity_df.index and t2_clean in similarity_df.columns:\n",
    "            similarity = float(similarity_df.at[t1_clean, t2_clean])  # Convert to native Python float\n",
    "            similarities.append(similarity)\n",
    "            \n",
    "            # Track the pair with the best similarity (lowest value)\n",
    "            if best_pair is None or similarity < best_pair[0]:\n",
    "                best_pair = (similarity, t1, t2)\n",
    "\n",
    "            # Track the pair with the worst similarity (highest value)\n",
    "            if worst_pair is None or similarity > worst_pair[0]:\n",
    "                worst_pair = (similarity, t1, t2)\n",
    "\n",
    "    if similarities:\n",
    "        best_similarity = min(similarities)\n",
    "        worst_similarity = max(similarities)\n",
    "        avg_similarity = sum(similarities) / len(similarities)\n",
    "        bucket_stats[bucket] = {\n",
    "            \"best\": best_similarity,\n",
    "            \"worst\": worst_similarity,\n",
    "            \"average\": avg_similarity,\n",
    "            \"best_pair\": best_pair,\n",
    "            \"worst_pair\": worst_pair,\n",
    "        }\n",
    "        \n",
    "        # Check if this bucket has the most trajectories\n",
    "        if len(trajectories) > max_bucket_size:\n",
    "            max_bucket_size = len(trajectories)\n",
    "            max_bucket = bucket\n",
    "    else:\n",
    "        # Handle missing trajectory pairs in the CSV\n",
    "        bucket_stats[bucket] = {\"best\": None, \"worst\": None, \"average\": None}\n",
    "\n",
    "# Display results for the bucket with the most trajectories\n",
    "if max_bucket:\n",
    "    print(f\"Bucket with the most trajectories: {max_bucket} ({max_bucket_size} trajectories)\")\n",
    "    best_similarity, best_t1, best_t2 = bucket_stats[max_bucket][\"best_pair\"]\n",
    "    worst_similarity, worst_t1, worst_t2 = bucket_stats[max_bucket][\"worst_pair\"]\n",
    "    print(f\"Most Similar Pair (Lowest similarity value): {best_similarity} between {best_t1} and {best_t2}\")\n",
    "    print(f\"Least Similar Pair (Highest similarity value): {worst_similarity} between {worst_t1} and {worst_t2}\")\n",
    "    print(f\"Average Similarity inside bucket '{max_bucket}': {bucket_stats[max_bucket]['average']:.2f}\")\n",
    "else:\n",
    "    print(\"No buckets with more than one trajectory found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CityHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest Bucket: 299718756038788115561691674111661080046\n",
      "Total Buckets: 446\n",
      "Buckets with more than one trajectory: 22\n",
      "Buckets with only one trajectory: 424\n",
      "Largest Bucket Size: 6\n",
      "Percentage of buckets with more than one trajectory: 4.93%\n",
      "Percentage of buckets with only one trajectory: 95.07%\n"
     ]
    }
   ],
   "source": [
    "import cityhash\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "PORTO_HASHED_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/hashed_data/disk/porto/\"\n",
    "PORTO_HASHED_TRAJECTORIES_FOLDER_META_FILE = f\"{PORTO_HASHED_TRAJECTORIES_OUTPUT_FOLDER}META-{porto_lsh_disk_current_parameters[\"number_of_trajectories\"]}.txt\"\n",
    "\n",
    "PORTO_FULL_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/porto/output/\"\n",
    "\n",
    "# Dictionary for the bucket system\n",
    "bucket_system = {}\n",
    "\n",
    "# Get filenames from the metafile\n",
    "files = mfh.read_meta_file(PORTO_HASHED_TRAJECTORIES_FOLDER_META_FILE)\n",
    "\n",
    "# Iterate through trajectory files and read their hashes\n",
    "for filename in files:\n",
    "    file_path = os.path.join(PORTO_HASHED_TRAJECTORIES_OUTPUT_FOLDER, filename)\n",
    "    \n",
    "    # Read the hashes for the trajectory\n",
    "    trajectory_hashes = fh.read_hash_file(file_path)\n",
    "    \n",
    "    # Iterate over each layer's hash\n",
    "    for layer_hash in trajectory_hashes:\n",
    "\n",
    "        if layer_hash == ['']:\n",
    "            continue\n",
    "            \n",
    "\n",
    "        # Convert the list of coordinates into a string\n",
    "        hash_string = \"\".join(map(str, layer_hash))\n",
    "\n",
    "\n",
    "        \n",
    "        # Use CityHash for creating a unique key\n",
    "        hash_key = cityhash.CityHash128(hash_string)\n",
    "        if hash_key == \"\":\n",
    "            print(filename, hash_string)\n",
    "        \n",
    "        # Place trajectory into the appropriate bucket\n",
    "        if hash_key not in bucket_system:\n",
    "            bucket_system[hash_key] = []\n",
    "        bucket_system[hash_key].append(filename)\n",
    "        \n",
    "        \n",
    "\n",
    "# Write the bucket system to a JSON file\n",
    "output_file_path = f\"../../results_hashed/buckets/disk/porto/porto_disk_diameter_{porto_lsh_disk_current_parameters['diameter']}_layers_{porto_lsh_disk_current_parameters['layers']}_disks_{porto_lsh_disk_current_parameters['num_disks']}_trajectories_{porto_lsh_disk_current_parameters['number_of_trajectories']}.json\"\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    json.dump(bucket_system, f)\n",
    "    \n",
    "# Analyze and display results\n",
    "total_buckets = len(bucket_system)\n",
    "buckets_with_multiple = sum(1 for trajectories in bucket_system.values() if len(trajectories) > 1)\n",
    "buckets_with_single = total_buckets - buckets_with_multiple\n",
    "largest_bucket_size = max(len(trajectories) for trajectories in bucket_system.values())\n",
    "largest_bucket = max(bucket_system, key=lambda key: len(bucket_system[key]))\n",
    "print(f\"Largest Bucket: {largest_bucket}\")\n",
    "\n",
    "print(f\"Total Buckets: {total_buckets}\")\n",
    "print(f\"Buckets with more than one trajectory: {buckets_with_multiple}\")\n",
    "print(f\"Buckets with only one trajectory: {buckets_with_single}\")\n",
    "print(f\"Largest Bucket Size: {largest_bucket_size}\")\n",
    "\n",
    "# Optional: Display distribution percentages\n",
    "multiple_bucket_percentage = (buckets_with_multiple / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "single_bucket_percentage = (buckets_with_single / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "\n",
    "print(f\"Percentage of buckets with more than one trajectory: {multiple_bucket_percentage:.2f}%\")\n",
    "print(f\"Percentage of buckets with only one trajectory: {single_bucket_percentage:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finds the largest bucket and outputs stats from this bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "PORTO_TRUE_SIMILARITY_FILE = \"../../results_true/similarity_values/porto/dtw/porto-dtw-3050.csv\"\n",
    "\n",
    "# Load the similarity matrix CSV\n",
    "similarity_df = pd.read_csv(PORTO_TRUE_SIMILARITY_FILE, index_col=0)\n",
    "\n",
    "# Results storage for statistics\n",
    "bucket_stats = {}\n",
    "\n",
    "# Filter buckets to process only those with more than one trajectory\n",
    "filtered_buckets = {bucket: trajectories for bucket, trajectories in bucket_system.items() if len(trajectories) > 1}\n",
    "\n",
    "# Track the bucket with the most trajectories\n",
    "max_bucket_size = 0\n",
    "max_bucket = None\n",
    "\n",
    "# Process each filtered bucket\n",
    "for bucket, trajectories in filtered_buckets.items():\n",
    "    similarities = []\n",
    "    best_pair = None\n",
    "    worst_pair = None\n",
    "\n",
    "    # Compute all pairwise similarities within the bucket\n",
    "    for t1, t2 in combinations(trajectories, 2):\n",
    "        # Strip `.txt` from the trajectory names\n",
    "        t1_clean = t1.replace('.txt', '')\n",
    "        t2_clean = t2.replace('.txt', '')\n",
    "\n",
    "        # Ensure correct row-column order (t1_clean should be lexicographically larger)\n",
    "        if t1_clean < t2_clean:\n",
    "            t1_clean, t2_clean = t2_clean, t1_clean  # Swap for correct matrix access\n",
    "\n",
    "        if t1_clean in similarity_df.index and t2_clean in similarity_df.columns:\n",
    "            similarity = float(similarity_df.at[t1_clean, t2_clean])  # Convert to native Python float\n",
    "            similarities.append(similarity)\n",
    "            \n",
    "            # Track the pair with the best similarity (lowest value)\n",
    "            if best_pair is None or similarity < best_pair[0]:\n",
    "                best_pair = (similarity, t1, t2)\n",
    "\n",
    "            # Track the pair with the worst similarity (highest value)\n",
    "            if worst_pair is None or similarity > worst_pair[0]:\n",
    "                worst_pair = (similarity, t1, t2)\n",
    "\n",
    "    if similarities:\n",
    "        best_similarity = min(similarities)\n",
    "        worst_similarity = max(similarities)\n",
    "        avg_similarity = sum(similarities) / len(similarities)\n",
    "        bucket_stats[bucket] = {\n",
    "            \"best\": best_similarity,\n",
    "            \"worst\": worst_similarity,\n",
    "            \"average\": avg_similarity,\n",
    "            \"best_pair\": best_pair,\n",
    "            \"worst_pair\": worst_pair,\n",
    "        }\n",
    "        \n",
    "        # Check if this bucket has the most trajectories\n",
    "        if len(trajectories) > max_bucket_size:\n",
    "            max_bucket_size = len(trajectories)\n",
    "            max_bucket = bucket\n",
    "    else:\n",
    "        # Handle missing trajectory pairs in the CSV\n",
    "        bucket_stats[bucket] = {\"best\": None, \"worst\": None, \"average\": None}\n",
    "\n",
    "# Display results for the bucket with the most trajectories\n",
    "if max_bucket:\n",
    "    print(f\"Bucket with the most trajectories: {max_bucket} ({max_bucket_size} trajectories)\")\n",
    "    best_similarity, best_t1, best_t2 = bucket_stats[max_bucket][\"best_pair\"]\n",
    "    worst_similarity, worst_t1, worst_t2 = bucket_stats[max_bucket][\"worst_pair\"]\n",
    "    print(f\"Most Similar Pair (Lowest similarity value): {best_similarity} between {best_t1} and {best_t2}\")\n",
    "    print(f\"Least Similar Pair (Highest similarity value): {worst_similarity} between {worst_t1} and {worst_t2}\")\n",
    "    print(f\"Average Similarity inside bucket '{max_bucket}': {bucket_stats[max_bucket]['average']:.2f}\")\n",
    "else:\n",
    "    print(\"No buckets with more than one trajectory found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket_system[83955211460528069735602233183555021590])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
