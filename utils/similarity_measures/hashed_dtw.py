import numpy as np
import pandas as pd
import collections as co
from traj_dist.distance import dtw as c_dtw

from multiprocessing import Pool

from utils.helpers.validators import is_invalid_hashed_trajectories


def cy_dtw_hashes(hashes: dict[str, list[list[list[float]]]]) -> pd.DataFrame:
    """
    Method for computing DTW similarity between all layers of trajectories in a given dataset using cython, and summing these similarities.

    Params
    ---
    trajectories : dict[str, list[list[list[float]]]]
        A dictionary containing the trajectories, where each key corresponds to multiple layers of trajectories.

    Returns
    ---
    A nxn pandas dataframe containing the pairwise summed similarities - sorted alphabetically
    """
    sorted_trajectories = co.OrderedDict(sorted(hashes.items()))
    num_trajectories = len(sorted_trajectories)

    M = np.zeros((num_trajectories, num_trajectories))

    for i, traj_i in enumerate(sorted_trajectories.keys()):
        for j, traj_j in enumerate(sorted_trajectories.keys()):
            total_dtw = 0  # Initialize total DTW similarity for this pair
            for layer_i, layer_j in zip(
                sorted_trajectories[traj_i], sorted_trajectories[traj_j]
            ):
                if is_invalid_hashed_trajectories(layer_i=layer_i, layer_j=layer_j):
                    continue
                X = np.array(layer_i)
                Y = np.array(layer_j)
                # Ensure both X and Y are not empty and have the correct shape
                if X.size > 0 and Y.size > 0 and X.ndim == 2 and Y.ndim == 2:
                    dtw = c_dtw(
                        X, Y
                    )  # Assuming c_dtw is defined elsewhere to calculate DTW similarity
                    total_dtw += dtw
            M[i, j] = total_dtw
            if i == j:
                break  # This optimizes by not recalculating for identical trajectories
    df = pd.DataFrame(
        M, index=sorted_trajectories.keys(), columns=sorted_trajectories.keys()
    )

    return df


def _fun_wrapper_hashes(args):
    x_layers, y_layers, j = args
    filtered_x_layers = [x for x in x_layers if x]  # Filter out empty lists
    filtered_y_layers = [y for y in y_layers if y]  # Filter out empty lists
    dtw_sum = sum(
        c_dtw(np.array(x), np.array(y))
        for x, y in zip(filtered_x_layers, filtered_y_layers)
    )
    return dtw_sum, j


def cy_dtw_hashes_pool(
    trajectories: dict[str, list[list[list[float]]]]
) -> pd.DataFrame:
    """
    Calculates the DTW similarity for trajectories with multiple layers, using a pool of processes for speedup.
    """
    sorted_trajectories = co.OrderedDict(sorted(trajectories.items()))
    num_trajectories = len(sorted_trajectories)

    M = np.zeros((num_trajectories, num_trajectories))

    with Pool(12) as pool:
        for i, traj_i_key in enumerate(sorted_trajectories.keys()):
            traj_i_layers = sorted_trajectories[traj_i_key]

            dtw_elements = pool.map(
                _fun_wrapper_hashes,
                [
                    (
                        traj_i_layers,
                        sorted_trajectories[traj_j_key],
                        j,
                    )
                    for j, traj_j_key in enumerate(sorted_trajectories.keys())
                    if i >= j
                ],
            )

            for dtw_sum, j in dtw_elements:
                M[i, j] = dtw_sum
                M[j, i] = dtw_sum  # Assuming DTW distance is symmetric

    df = pd.DataFrame(
        M, index=sorted_trajectories.keys(), columns=sorted_trajectories.keys()
    )

    return df
