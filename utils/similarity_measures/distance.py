import numpy as np
import pandas as pd
import collections as co

from multiprocessing import Pool
import timeit as ti
import time

from collections import OrderedDict
from .py.edit_distance import edit_distance as p_ed
from .py.edit_distance_penalty import edit_distance_penalty as p_edp
from .py.hashed_dtw import dtw as p_dtw
from utils.similarity_measures.frechet import cy_frechet_pool, cy_frechet


def py_edit_distance(hashes: dict[str, list[list[str]]]) -> pd.DataFrame:
    """
    Method for computing Edit distance similarity between hashes generated by the grid and disk LSH using python.

    Params
    ---
    hashes : dict[str, list[list[str]]]
        A dictionary containing the trajectory hashes

    Returns
    ---
    A NxN pandas dataframe containing the pairwise similarities
    """

    sorted_hashes = co.OrderedDict(sorted(hashes.items()))
    num_hashes = len(sorted_hashes)

    M = np.zeros((num_hashes, num_hashes))
    for i, hash_i in enumerate(sorted_hashes.keys()):
        for j, hash_j in enumerate(sorted_hashes.keys()):
            X = np.array(sorted_hashes[hash_i], dtype=object)
            Y = np.array(sorted_hashes[hash_j], dtype=object)
            e_dist = p_ed(X, Y)[0]
            M[i, j] = e_dist
            if i == j:
                break

    df = pd.DataFrame(M, index=sorted_hashes.keys(), columns=sorted_hashes.keys())

    return df


def py_edit_distance_penalty(hashes: dict[str, list[list[str]]]) -> pd.DataFrame:
    """Test method Edit distance penalty"""
    sorted_hashes = co.OrderedDict(sorted(hashes.items()))
    num_hashes = len(sorted_hashes)

    M = np.zeros((num_hashes, num_hashes))
    for i, hash_i in enumerate(sorted_hashes.keys()):
        for j, hash_j in enumerate(sorted_hashes.keys()):
            X = np.array(sorted_hashes[hash_i], dtype=object)
            Y = np.array(sorted_hashes[hash_j], dtype=object)
            e_dist = p_edp(X, Y)[0]
            M[i, j] = e_dist
            if i == j:
                break

    df = pd.DataFrame(M, index=sorted_hashes.keys(), columns=sorted_hashes.keys())

    return df


def _fun_wrapper_edpp(args):
    x, y, j = args
    e_dist = p_edp(x, y)[0]
    return e_dist, j


def py_edit_distance_penalty_parallell(
    hashes: dict[str, list[list[str]]]
) -> pd.DataFrame:
    """Edit distance penalty for hashes computed in parallell"""

    sorted_hashes = co.OrderedDict(sorted(hashes.items()))
    num_hashes = len(sorted_hashes)

    M = np.zeros((num_hashes, num_hashes))
    pool = Pool(12)

    for i, hash_i in enumerate(sorted_hashes.keys()):
        elements = pool.map(
            _fun_wrapper_edpp,
            [
                (
                    np.array(sorted_hashes[hash_i], dtype=object),
                    np.array(sorted_hashes[traj_j], dtype=object),
                    j,
                )
                for j, traj_j in enumerate(sorted_hashes.keys())
                if i >= j
            ],
        )

        for element in elements:
            M[i, element[1]] = element[0]

    df = pd.DataFrame(M, index=sorted_hashes.keys(), columns=sorted_hashes.keys())

    return df


def py_dtw(hashes: dict[str, list[list[float]]]) -> pd.DataFrame:
    """Coordinate dtw as hashes"""
    sorted_hashes = co.OrderedDict(sorted(hashes.items()))
    num_hashes = len(sorted_hashes)

    M = np.zeros((num_hashes, num_hashes))
    for i, hash_i in enumerate(sorted_hashes.keys()):
        for j, hash_j in enumerate(sorted_hashes.keys()):
            X = np.array(sorted_hashes[hash_i], dtype=object)
            Y = np.array(sorted_hashes[hash_j], dtype=object)
            e_dist = p_dtw(X, Y)
            M[i, j] = e_dist
            if i == j:
                break

    df = pd.DataFrame(M, index=sorted_hashes.keys(), columns=sorted_hashes.keys())

    return df


def transform_frechet_disk(hashes: dict[str, list[list[float]]]) -> OrderedDict:
    """Transforms the numerical disk hashes to a format that fits the true frechet similarity measure (non numpy input)"""
    transformed_data = OrderedDict()
    for key, layer in hashes.items():
        transformed_points = []
        for points in layer:
            transformed_traj = [point.tolist() for point in points]
            for point in transformed_traj:
                transformed_points.append(point)
        transformed_data[key] = transformed_points
    return transformed_data


def frechet_disk(hashes: dict[str, list[list[float]]]) -> pd.DataFrame:
    """Frechet distance for disk hashes(Used for correlation computation due to parallell jobs)"""
    transformed_data = transform_frechet_disk(hashes)
    return cy_frechet(transformed_data)


def frechet_disk_parallell(hashes: dict[str, list[list[float]]]) -> pd.DataFrame:
    """Frechet distance for disk hashes computed in parallell"""
    transformed_data = transform_frechet_disk(hashes)
    return cy_frechet_pool(transformed_data)


def _fun_wrapper_dtw(args):
    x, y, j = args
    e_dist = p_dtw(x, y)
    return e_dist, j


def py_dtw_parallell(hashes: dict[str, list[list[str]]]) -> pd.DataFrame:
    """Edit distance penalty for hashes computed in parallell"""

    sorted_hashes = co.OrderedDict(sorted(hashes.items()))
    num_hashes = len(sorted_hashes)

    M = np.zeros((num_hashes, num_hashes))
    pool = Pool(12)

    for i, hash_i in enumerate(sorted_hashes.keys()):
        elements = pool.map(
            _fun_wrapper_dtw,
            [
                (
                    np.array(sorted_hashes[hash_i], dtype=object),
                    np.array(sorted_hashes[traj_j], dtype=object),
                    j,
                )
                for j, traj_j in enumerate(sorted_hashes.keys())
                if i >= j
            ],
        )

        for element in elements:
            M[i, element[1]] = element[0]

    df = pd.DataFrame(M, index=sorted_hashes.keys(), columns=sorted_hashes.keys())

    return df


def measure_py_ed(args):
    """
    Method for measuring time efficiency using py_ed

    Params
    ---
    args : (hashes: dict[str, list[list[str]]], number: int, repeat : int) list
    """
    hashes, number, repeat = args

    measures = ti.repeat(
        lambda: py_edit_distance(hashes),
        number=number,
        repeat=repeat,
        timer=time.process_time,
    )

    return measures


if __name__ == "__main__":
    d = {
        "a": [["a", "b", "c", "d"], ["a", "b", "c"]],
        "b": [["a", "c", "d"], ["a", "b", "d"]],
    }
    print(py_edit_distance(d))

    t = ti.repeat(lambda: py_edit_distance(d), repeat=3, number=10000)
    print(t)
