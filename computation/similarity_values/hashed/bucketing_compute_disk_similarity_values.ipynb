{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for computing hashes, buckets and similarity values for the disk scheme. \n",
    "\n",
    "Utilizes the disk scheme\n",
    "\n",
    "Incorporates:\n",
    "* Hashing of trajectories using disk scheme\n",
    "* Bucketing of hashes made from disk scheme\n",
    "* Similarity computation between trajectories within buckets.\n",
    "    * Both for DTW and Frechet\n",
    "* Analysis of the produced bucket system\n",
    "\n",
    "Produces:\n",
    "* JSON file containing buckets\n",
    "* Similarity values for trajectories within buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(target_folder=\"masteroppgave\"):\n",
    "    \"\"\"Find the absolute path of a folder by searching upward.\"\"\"\n",
    "    currentdir = os.path.abspath(\"__file__\")  # Get absolute script path\n",
    "    while True:\n",
    "        if os.path.basename(currentdir) == target_folder:\n",
    "            return currentdir  # Found the target folder\n",
    "        parentdir = os.path.dirname(currentdir)\n",
    "        if parentdir == currentdir:  # Stop at filesystem root\n",
    "            return None\n",
    "        currentdir = parentdir  # Move one level up\n",
    "\n",
    "# Example usage\n",
    "project_root = find_project_root(\"masteroppgave\")\n",
    "\n",
    "if project_root:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Project root found: {project_root}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find 'masteroppgave' directory\")\n",
    "\n",
    "from computation.similarity import *\n",
    "from utils.helpers.bucket_evaluation import *\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY = \"rome\" # \"rome\" or \"porto\"\n",
    "MEASURE = \"dtw\" # \"dtw\" or \"frechet\"\n",
    "DIAMETER = 2 # Diameter of the disks\n",
    "LAYERS = 2 # Number of layers\n",
    "DISKS = 50 # NUmber of disks\n",
    "SIZE = 500 #How many trajectories to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the true similarity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"../../../results_true/similarity_values/{CITY}/{MEASURE}/{CITY}-{MEASURE}-{SIZE}.csv\"\n",
    "\n",
    "# Read CSV, telling pandas to take the first column as the row labels:\n",
    "true_sim_matrix_df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Function to convert values to float if possible\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value\n",
    "\n",
    "# Apply the function to each cell in the DataFrame\n",
    "true_sim_matrix_df = true_sim_matrix_df.map(convert_to_float)\n",
    "true_sim_matrix_df = (true_sim_matrix_df + true_sim_matrix_df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hashes with disk scheme, bucket system and similarity values for the given city and measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_similarities, bucket_system = generate_disk_hash_similarity_with_bucketing(\n",
    "    city=CITY, diameter=DIAMETER, layers=LAYERS, disks=DISKS, measure=MEASURE, size=SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_similarities.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_buckets = len(bucket_system)\n",
    "buckets_with_multiple = sum(1 for trajectories in bucket_system.values() if len(trajectories) > 1)\n",
    "buckets_with_single = total_buckets - buckets_with_multiple\n",
    "largest_bucket_size = max(len(trajectories) for trajectories in bucket_system.values())\n",
    "largest_bucket = max(bucket_system, key=lambda key: len(bucket_system[key]))\n",
    "smallest_bucket = min(bucket_system, key=lambda key: len(bucket_system[key]))\n",
    "smallest_bucket_size = len(bucket_system[smallest_bucket])\n",
    "\n",
    "\n",
    "print(f\"Total Buckets: {total_buckets}\")\n",
    "print(f\"Largest Bucket(id): {largest_bucket}\")\n",
    "print(f\"Buckets with more than one trajectory: {buckets_with_multiple}\")\n",
    "print(f\"Buckets with only one trajectory: {buckets_with_single}\")\n",
    "print(f\"Largest Bucket Size: {largest_bucket_size}\")\n",
    "print(\"smallest bucket: \", smallest_bucket_size)\n",
    "\n",
    "# Optional: Display distribution percentages\n",
    "multiple_bucket_percentage = (buckets_with_multiple / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "single_bucket_percentage = (buckets_with_single / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "\n",
    "print(f\"Percentage of buckets with more than one trajectory: {multiple_bucket_percentage:.2f}%\")\n",
    "print(f\"Percentage of buckets with only one trajectory: {single_bucket_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# print(bucket_system[largest_bucket])\n",
    "# print(bucket_system[smallest_bucket])\n",
    "\n",
    "# for key, value in bucket_system.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP, FP, FN, PRECISION, RECALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket system statistics for city: rome, measure: dtw, diameter: 2, layers: 2, disks: 50, size: 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold = 1.0</th>\n",
       "      <th>Threshold = 2.0</th>\n",
       "      <th>Threshold = 3.0</th>\n",
       "      <th>Threshold = 4.0</th>\n",
       "      <th>Threshold = 5.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.173497</td>\n",
       "      <td>0.543895</td>\n",
       "      <td>0.760954</td>\n",
       "      <td>0.881470</td>\n",
       "      <td>0.943420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.965468</td>\n",
       "      <td>0.915265</td>\n",
       "      <td>0.887075</td>\n",
       "      <td>0.868249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.295684</td>\n",
       "      <td>0.695808</td>\n",
       "      <td>0.831007</td>\n",
       "      <td>0.884263</td>\n",
       "      <td>0.904275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
       "Precision         0.173497         0.543895         0.760954         0.881470   \n",
       "Recall            0.999831         0.965468         0.915265         0.887075   \n",
       "F1 Score          0.295684         0.695808         0.831007         0.884263   \n",
       "\n",
       "           Threshold = 5.0  \n",
       "Precision         0.943420  \n",
       "Recall            0.868249  \n",
       "F1 Score          0.904275  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THRESHOLD = 5\n",
    "import numpy as np # type: ignore\n",
    "THRESHOLDS = np.arange(1, 6.0, 1)  # Generates [0.5, 1.0, 1.5, ..., 5.5]\n",
    "\n",
    "results = {\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for treshold in THRESHOLDS:\n",
    "\n",
    "    #Variables\n",
    "    all_trajectory_names = list(hashed_similarities.keys()) # All trajectory names\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    precision = 0 \n",
    "    recall = 0\n",
    "    f1_Score = 0\n",
    "\n",
    "    # Loop through all trajectory names\n",
    "    for trajectory in all_trajectory_names:\n",
    "        \n",
    "        # Pred and ground truth\n",
    "        predicted_similar = find_predicted_similar_trajectories(trajectory, bucket_system)\n",
    "        ground_truth = get_nearest_neighbour_under_threshold(trajectory, treshold, true_sim_matrix_df).index.to_list()  \n",
    "        true_positives += calculate_true_positives(predicted_similar, ground_truth)\n",
    "        false_positives += calculate_false_positives(predicted_similar, ground_truth)\n",
    "        false_negatives += calculate_false_negatives(predicted_similar, ground_truth)\n",
    "        \n",
    "    # Calculate precision and recall\n",
    "    precision = compute_bucket_system_precision(true_positives, false_positives)\n",
    "    recall = compute_bucket_system_recall(true_positives, false_negatives)\n",
    "    f1_score = compute_bucket_system_f1_score(precision, recall)\n",
    "\n",
    "    # print(f\"Bucket system statistics for city: {CITY}, measure: {MEASURE}, diameter: {DIAMETER}, layers: {LAYERS}, disks: {DISKS}, size: {SIZE}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1-Score: {f1_score}\")\n",
    "    \n",
    "\n",
    "    results[\"Precision\"].append(precision)\n",
    "    results[\"Recall\"].append(recall)\n",
    "    results[\"F1 Score\"].append(f1_score)\n",
    "    \n",
    "    \n",
    "print(f\"Bucket system statistics for city: {CITY}, measure: {MEASURE}, diameter: {DIAMETER}, layers: {LAYERS}, disks: {DISKS}, size: {SIZE}\")\n",
    "# Create DataFrame with thresholds as columns and metrics as row indexes\n",
    "df = pd.DataFrame(results, index=[f\"Threshold = {t}\" for t in THRESHOLDS]).T\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell for performing several executions with different parameter values, writes all results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=5, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.195046         0.647059         0.858617         0.958720   \n",
      "Recall            0.100372         0.101145         0.093028         0.087448   \n",
      "F1 Score          0.132539         0.174944         0.167869         0.160276   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.986584  \n",
      "Recall            0.083181  \n",
      "F1 Score          0.153426  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=5, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.184008         0.619461         0.834297         0.944123   \n",
      "Recall            0.101434         0.103726         0.096830         0.092248   \n",
      "F1 Score          0.130777         0.177698         0.173521         0.168074   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.978805  \n",
      "Recall            0.088402  \n",
      "F1 Score          0.162158  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=5, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.185437         0.623301         0.839806         0.943689   \n",
      "Recall            0.101434         0.103565         0.096718         0.091495   \n",
      "F1 Score          0.131136         0.177618         0.173460         0.166817   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.977670  \n",
      "Recall            0.087619  \n",
      "F1 Score          0.160824  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=10, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.185078         0.624031         0.838178         0.944767   \n",
      "Recall            0.101434         0.103888         0.096718         0.091778   \n",
      "F1 Score          0.131046         0.178122         0.173425         0.167303   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.977713  \n",
      "Recall            0.087793  \n",
      "F1 Score          0.161118  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=10, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.183831         0.622714         0.840231         0.944177   \n",
      "Recall            0.101434         0.104372         0.097613         0.092342   \n",
      "F1 Score          0.130732         0.178779         0.174906         0.168232   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.979788  \n",
      "Recall            0.088576  \n",
      "F1 Score          0.162464  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=10, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.181732         0.615604         0.836346         0.944814   \n",
      "Recall            0.101434         0.104372         0.098284         0.093472   \n",
      "F1 Score          0.130198         0.178483         0.175897         0.170114   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.980019  \n",
      "Recall            0.089620  \n",
      "F1 Score          0.164222  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=15, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.184185         0.622951         0.841851         0.945034   \n",
      "Recall            0.101434         0.104210         0.097613         0.092248   \n",
      "F1 Score          0.130822         0.178552         0.174941         0.168089   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.979749  \n",
      "Recall            0.088402  \n",
      "F1 Score          0.162171  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=15, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.179511         0.609962         0.828947         0.937970   \n",
      "Recall            0.101434         0.104694         0.098619         0.093943   \n",
      "F1 Score          0.129623         0.178714         0.176268         0.170781   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.977444  \n",
      "Recall            0.090490  \n",
      "F1 Score          0.165645  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=1.6, Layers=15, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.180189         0.612264         0.831132         0.939623   \n",
      "Recall            0.101434         0.104694         0.098507         0.093754   \n",
      "F1 Score          0.129800         0.178813         0.176138         0.170497   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.977358  \n",
      "Recall            0.090142  \n",
      "F1 Score          0.165060  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=5, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.183301         0.620921         0.835893         0.940499   \n",
      "Recall            0.101434         0.104372         0.097389         0.092248   \n",
      "F1 Score          0.130598         0.178705         0.174453         0.168017   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.976008  \n",
      "Recall            0.088489  \n",
      "F1 Score          0.162266  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=5, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.173321         0.592559         0.810345         0.926497   \n",
      "Recall            0.101434         0.105340         0.099849         0.096108   \n",
      "F1 Score          0.127973         0.178880         0.177791         0.174150   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.970054  \n",
      "Recall            0.093013  \n",
      "F1 Score          0.169750  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=5, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.172072         0.588288         0.806306         0.922523   \n",
      "Recall            0.101434         0.105340         0.100073         0.096390   \n",
      "F1 Score          0.127631         0.178684         0.178047         0.174543   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.968468  \n",
      "Recall            0.093535  \n",
      "F1 Score          0.170594  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=10, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.173636         0.592727         0.807273         0.923636   \n",
      "Recall            0.101434         0.105178         0.099290         0.095637   \n",
      "F1 Score          0.128059         0.178655         0.176831         0.173327   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.971818  \n",
      "Recall            0.093013  \n",
      "F1 Score          0.169777  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=10, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.171763         0.587230         0.803058         0.919964   \n",
      "Recall            0.101434         0.105340         0.099849         0.096296   \n",
      "F1 Score          0.127546         0.178635         0.177614         0.174343   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.968525  \n",
      "Recall            0.093709  \n",
      "F1 Score          0.170885  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=10, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.170841         0.585868         0.803220         0.921288   \n",
      "Recall            0.101434         0.105662         0.100408         0.096955   \n",
      "F1 Score          0.127291         0.179035         0.178502         0.175446   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.967800  \n",
      "Recall            0.094144  \n",
      "F1 Score          0.171596  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=15, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.171917         0.588659         0.803780         0.921692   \n",
      "Recall            0.101434         0.105501         0.099849         0.096390   \n",
      "F1 Score          0.127589         0.178933         0.177632         0.174528   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.969397  \n",
      "Recall            0.093709  \n",
      "F1 Score          0.170898  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=15, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.171454         0.587971         0.805206         0.922801   \n",
      "Recall            0.101434         0.105662         0.100296         0.096767   \n",
      "F1 Score          0.127461         0.179133         0.178374         0.175165   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.969479  \n",
      "Recall            0.093970  \n",
      "F1 Score          0.171333  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.0, Layers=15, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.170232         0.582888         0.799465         0.918895   \n",
      "Recall            0.101434         0.105501         0.100296         0.097049   \n",
      "F1 Score          0.127121         0.178664         0.178233         0.175557   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.967023  \n",
      "Recall            0.094405  \n",
      "F1 Score          0.172017  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=5, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.166667         0.575044         0.788831         0.911867   \n",
      "Recall            0.101434         0.106307         0.101079         0.098367   \n",
      "F1 Score          0.126114         0.179442         0.179196         0.177578   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.964223  \n",
      "Recall            0.096145  \n",
      "F1 Score          0.174856  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=5, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.165225         0.570934         0.785467         0.908304   \n",
      "Recall            0.101434         0.106469         0.101526         0.098837   \n",
      "F1 Score          0.125699         0.179470         0.179811         0.178276   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.961938  \n",
      "Recall            0.096755  \n",
      "F1 Score          0.175824  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=5, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.164372         0.566265         0.783133         0.905336   \n",
      "Recall            0.101434         0.106146         0.101750         0.099026   \n",
      "F1 Score          0.125452         0.178780         0.180100         0.178524   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.959552  \n",
      "Recall            0.097016  \n",
      "F1 Score          0.176215  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=10, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.165368         0.569697         0.785281         0.905628   \n",
      "Recall            0.101434         0.106146         0.101414         0.098461   \n",
      "F1 Score          0.125741         0.178950         0.179631         0.177612   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.959307  \n",
      "Recall            0.096407  \n",
      "F1 Score          0.175206  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=10, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.164372         0.567986         0.783133         0.905336   \n",
      "Recall            0.101434         0.106469         0.101750         0.099026   \n",
      "F1 Score          0.125452         0.179323         0.180100         0.178524   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.959552  \n",
      "Recall            0.097016  \n",
      "F1 Score          0.176215  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=10, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.163808         0.566895         0.783019         0.904803   \n",
      "Recall            0.101434         0.106630         0.102085         0.099308   \n",
      "F1 Score          0.125287         0.179498         0.180622         0.178973   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.958834  \n",
      "Recall            0.097277  \n",
      "F1 Score          0.176633  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=15, Disks=20, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.164940         0.568221         0.783247         0.905872   \n",
      "Recall            0.101434         0.106146         0.101414         0.098743   \n",
      "F1 Score          0.125617         0.178877         0.179577         0.178076   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.959413  \n",
      "Recall            0.096668  \n",
      "F1 Score          0.175638  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=15, Disks=50, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.164940         0.568221         0.784974         0.905872   \n",
      "Recall            0.101434         0.106146         0.101638         0.098743   \n",
      "F1 Score          0.125617         0.178877         0.179973         0.178076   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.959413  \n",
      "Recall            0.096668  \n",
      "F1 Score          0.175638  \n",
      "--------------------------------------------------------------------------------\n",
      "Generating disk hash similarity with bucketing\n",
      "Computing dataset hashes\n",
      "Placing hashes into buckets\n",
      "Computing hash similarity within buckets\n",
      "Completed: City=rome, Measure=dtw, Diameter=2.5, Layers=15, Disks=70, Size=50\n",
      "           Threshold = 1.0  Threshold = 2.0  Threshold = 3.0  Threshold = 4.0  \\\n",
      "Precision         0.163668         0.566410         0.783205         0.904884   \n",
      "Recall            0.101434         0.106630         0.102197         0.099402   \n",
      "F1 Score          0.125246         0.179473         0.180802         0.179127   \n",
      "\n",
      "           Threshold = 5.0  \n",
      "Precision         0.958869  \n",
      "Recall            0.097364  \n",
      "F1 Score          0.176777  \n",
      "--------------------------------------------------------------------------------\n",
      "All results saved to bucket_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "# Define parameter ranges\n",
    "DIAMETER_VALUES = [1.6, 2.0, 2.5]  # Example diameters\n",
    "LAYERS_VALUES = [5, 10, 15]  # Example layers\n",
    "DISKS_VALUES = [20, 50, 70]  # Example disk counts\n",
    "SIZE_VALUES = [50]  # Example dataset sizes\n",
    "\n",
    "# Define similarity thresholds\n",
    "THRESHOLDS = np.arange(1, 6.0, 1)  # Generates [1, 2, 3, 4, 5]\n",
    "\n",
    "# Prepare CSV file for results\n",
    "csv_filename = \"bucket_evaluation_results.csv\"\n",
    "\n",
    "# Open CSV file and write headers\n",
    "with open(csv_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"City\", \"Measure\", \"Diameter\", \"Layers\", \"Disks\", \"Size\", \n",
    "                     \"Threshold\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "    \n",
    "    # Loop through parameter combinations\n",
    "    for DIAMETER, LAYERS, DISKS, SIZE in itertools.product(DIAMETER_VALUES, LAYERS_VALUES, DISKS_VALUES, SIZE_VALUES):\n",
    "        \n",
    "        # Generate bucket system\n",
    "        hashed_similarities, bucket_system = generate_disk_hash_similarity_with_bucketing(\n",
    "            city=CITY, diameter=DIAMETER, layers=LAYERS, disks=DISKS, measure=MEASURE, size=SIZE\n",
    "        )\n",
    "\n",
    "        # Store results for this configuration\n",
    "        results = {\"Precision\": [], \"Recall\": [], \"F1 Score\": []}\n",
    "\n",
    "        # Loop through each threshold\n",
    "        for threshold in THRESHOLDS:\n",
    "            all_trajectory_names = list(hashed_similarities.keys())  # List of trajectories\n",
    "            true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "\n",
    "            # Compute precision, recall, and F1 score\n",
    "            for trajectory in all_trajectory_names:\n",
    "                predicted_similar = find_predicted_similar_trajectories(trajectory, bucket_system)\n",
    "                ground_truth = get_nearest_neighbour_under_threshold(trajectory, threshold, true_sim_matrix_df).index.to_list()\n",
    "                true_positives += calculate_true_positives(predicted_similar, ground_truth)\n",
    "                false_positives += calculate_false_positives(predicted_similar, ground_truth)\n",
    "                false_negatives += calculate_false_negatives(predicted_similar, ground_truth)\n",
    "\n",
    "            # Compute final scores\n",
    "            precision = compute_bucket_system_precision(true_positives, false_positives)\n",
    "            recall = compute_bucket_system_recall(true_positives, false_negatives)\n",
    "            f1_score = compute_bucket_system_f1_score(precision, recall)\n",
    "\n",
    "            # Save to dictionary\n",
    "            results[\"Precision\"].append(precision)\n",
    "            results[\"Recall\"].append(recall)\n",
    "            results[\"F1 Score\"].append(f1_score)\n",
    "\n",
    "            # Write results to CSV\n",
    "            writer.writerow([CITY, MEASURE, DIAMETER, LAYERS, DISKS, SIZE, \n",
    "                             threshold, precision, recall, f1_score])\n",
    "\n",
    "        # Print summary for this configuration\n",
    "        print(f\"Completed: City={CITY}, Measure={MEASURE}, Diameter={DIAMETER}, Layers={LAYERS}, Disks={DISKS}, Size={SIZE}\")\n",
    "        print(pd.DataFrame(results, index=[f\"Threshold = {t}\" for t in THRESHOLDS]).T)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "print(f\"All results saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
