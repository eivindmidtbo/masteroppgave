{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for placing the hashed trajectories into buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root found: c:\\Users\\eivin\\dev\\JoonEndreLSH\\masteroppgave\n"
     ]
    }
   ],
   "source": [
    "# Importing nescessary modules\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import timeit as ti\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def find_project_root(target_folder=\"masteroppgave\"):\n",
    "    \"\"\"Find the absolute path of a folder by searching upward.\"\"\"\n",
    "    currentdir = os.path.abspath(\"__file__\")  # Get absolute script path\n",
    "    while True:\n",
    "        if os.path.basename(currentdir) == target_folder:\n",
    "            return currentdir  # Found the target folder\n",
    "        parentdir = os.path.dirname(currentdir)\n",
    "        if parentdir == currentdir:  # Stop at filesystem root\n",
    "            return None\n",
    "        currentdir = parentdir  # Move one level up\n",
    "\n",
    "# Example usage\n",
    "project_root = find_project_root(\"masteroppgave\")\n",
    "\n",
    "if project_root:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Project root found: {project_root}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find 'masteroppgave' directory\")\n",
    "\n",
    "from utils.helpers.save_trajectory import save_trajectory_hashes\n",
    "from utils.helpers import file_handler as fh\n",
    "from utils.helpers import metafile_handler as mfh\n",
    "from schemes.lsh_disk import DiskLSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CityHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 9708320301858696672609084931132828922: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "Bucket 29427960669666322764846783719845337828: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "Bucket 221877929939281149391141385146078258469: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "Bucket 48727781184623431333552088798654009724: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "Bucket 38805828736283655287654616728968258560: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "Largest Bucket: 9708320301858696672609084931132828922\n",
      "Total Buckets: 5\n",
      "Buckets with more than one trajectory: 5\n",
      "Buckets with only one trajectory: 0\n",
      "Largest Bucket Size: 50\n",
      "Percentage of buckets with more than one trajectory: 100.00%\n",
      "Percentage of buckets with only one trajectory: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import cityhash\n",
    "import os\n",
    "from constants import NUMBER_OF_TRAJECTORIES\n",
    "\n",
    "# Paths\n",
    "ROME_HASHED_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/hashed_data/grid/rome/\"\n",
    "ROME_HASHED_TRAJECTORIES_FOLDER_META_FILE = f\"{ROME_HASHED_TRAJECTORIES_OUTPUT_FOLDER}META-{NUMBER_OF_TRAJECTORIES}.txt\"\n",
    "\n",
    "ROME_FULL_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/rome/output/\"\n",
    "\n",
    "# Dictionary for the bucket system\n",
    "bucket_system = {}\n",
    "\n",
    "# Get filenames from the metafile\n",
    "files = mfh.read_meta_file(ROME_HASHED_TRAJECTORIES_FOLDER_META_FILE)\n",
    "\n",
    "ABU_HASH = []\n",
    "\n",
    "# Iterate through trajectory files and read their hashes\n",
    "for filename in files:\n",
    "    file_path = os.path.join(ROME_HASHED_TRAJECTORIES_OUTPUT_FOLDER, filename)\n",
    "    \n",
    "    # Read the hashes for the trajectory\n",
    "    trajectory_hashes = fh.read_hash_file(file_path)\n",
    "    \n",
    "    # Iterate over each layer's hash\n",
    "    \n",
    "    for layer_hash in trajectory_hashes:\n",
    "        # Convert the list of coordinates into a string\n",
    "        hash_string = \"_\".join(map(str, layer_hash))\n",
    "        \n",
    "        # Use CityHash for creating a unique key\n",
    "        hash_key = cityhash.CityHash128(hash_string)\n",
    "        \n",
    "        # Place trajectory into the appropriate bucket\n",
    "        if hash_key not in bucket_system:\n",
    "            bucket_system[hash_key] = []\n",
    "        bucket_system[hash_key].append(filename)\n",
    "        if filename == \"R_ABU.txt\":\n",
    "            ABU_HASH.append(hash_key)\n",
    "\n",
    "\n",
    "# Print the contents of the buckets for \"R_ABU.txt\"\n",
    "for hash_key in ABU_HASH:\n",
    "    print(f\"Bucket {hash_key}: {bucket_system[hash_key]}\")\n",
    "   \n",
    "\n",
    "\n",
    "# Analyze and display results\n",
    "\n",
    "total_buckets = len(bucket_system)\n",
    "buckets_with_multiple = sum(1 for trajectories in bucket_system.values() if len(trajectories) > 1)\n",
    "buckets_with_single = total_buckets - buckets_with_multiple\n",
    "largest_bucket_size = max(len(trajectories) for trajectories in bucket_system.values())\n",
    "largest_bucket = max(bucket_system, key=lambda key: len(bucket_system[key]))\n",
    "print(f\"Largest Bucket: {largest_bucket}\")\n",
    "\n",
    "print(f\"Total Buckets: {total_buckets}\")\n",
    "print(f\"Buckets with more than one trajectory: {buckets_with_multiple}\")\n",
    "print(f\"Buckets with only one trajectory: {buckets_with_single}\")\n",
    "print(f\"Largest Bucket Size: {largest_bucket_size}\")\n",
    "\n",
    "# Optional: Display distribution percentages\n",
    "multiple_bucket_percentage = (buckets_with_multiple / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "single_bucket_percentage = (buckets_with_single / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "\n",
    "print(f\"Percentage of buckets with more than one trajectory: {multiple_bucket_percentage:.2f}%\")\n",
    "print(f\"Percentage of buckets with only one trajectory: {single_bucket_percentage:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABU.txt buckets: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "ABU.txt buckets: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "ABU.txt buckets: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "ABU.txt buckets: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n",
      "ABU.txt buckets: ['R_CAV.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_DOY.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_COQ.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVF.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_AVB.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_AKK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_DUB.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_EDX.txt', 'R_CIV.txt', 'R_ABU.txt', 'R_BDC.txt', 'R_BML.txt', 'R_ECP.txt', 'R_AZS.txt', 'R_BFS.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_CRC.txt', 'R_ARC.txt', 'R_DDN.txt', 'R_DAQ.txt', 'R_CNH.txt']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ABU_HASH)):\n",
    "    print(f\"ABU.txt buckets: {bucket_system[ABU_HASH[i]]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket with the most trajectories: 9708320301858696672609084931132828922 (50 trajectories)\n",
      "Most Similar Pair (Lowest similarity value): 0.1406260090323557 between R_DUB.txt and R_ECP.txt\n",
      "Least Similar Pair (Highest similarity value): 10.327691364230498 between R_AVD.txt and R_BDC.txt\n",
      "Average Similarity inside bucket '9708320301858696672609084931132828922': 2.25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "ROME_TRUE_SIMILARITY_FILE = \"../../results_true/similarity_values/rome/dtw/rome-dtw-3050.csv\"\n",
    "\n",
    "# Load the similarity matrix CSV\n",
    "similarity_df = pd.read_csv(ROME_TRUE_SIMILARITY_FILE, index_col=0)\n",
    "\n",
    "# Results storage for statistics\n",
    "bucket_stats = {}\n",
    "\n",
    "# Filter buckets to process only those with more than one trajectory\n",
    "filtered_buckets = {bucket: trajectories for bucket, trajectories in bucket_system.items() if len(trajectories) > 1}\n",
    "\n",
    "# Track the bucket with the most trajectories\n",
    "max_bucket_size = 0\n",
    "max_bucket = None\n",
    "\n",
    "# Process each filtered bucket\n",
    "for bucket, trajectories in filtered_buckets.items():\n",
    "    similarities = []\n",
    "    best_pair = None\n",
    "    worst_pair = None\n",
    "\n",
    "    # Compute all pairwise similarities within the bucket\n",
    "    for t1, t2 in combinations(trajectories, 2):\n",
    "        # Strip `.txt` from the trajectory names\n",
    "        t1_clean = t1.replace('.txt', '')\n",
    "        t2_clean = t2.replace('.txt', '')\n",
    "\n",
    "        # Ensure correct row-column order (t1_clean should be lexicographically larger)\n",
    "        if t1_clean < t2_clean:\n",
    "            t1_clean, t2_clean = t2_clean, t1_clean  # Swap for correct matrix access\n",
    "\n",
    "        if t1_clean in similarity_df.index and t2_clean in similarity_df.columns:\n",
    "            similarity = float(similarity_df.at[t1_clean, t2_clean])  # Convert to native Python float\n",
    "            similarities.append(similarity)\n",
    "            \n",
    "            # Track the pair with the best similarity (lowest value)\n",
    "            if best_pair is None or similarity < best_pair[0]:\n",
    "                best_pair = (similarity, t1, t2)\n",
    "\n",
    "            # Track the pair with the worst similarity (highest value)\n",
    "            if worst_pair is None or similarity > worst_pair[0]:\n",
    "                worst_pair = (similarity, t1, t2)\n",
    "\n",
    "    if similarities:\n",
    "        best_similarity = min(similarities)\n",
    "        worst_similarity = max(similarities)\n",
    "        avg_similarity = sum(similarities) / len(similarities)\n",
    "        bucket_stats[bucket] = {\n",
    "            \"best\": best_similarity,\n",
    "            \"worst\": worst_similarity,\n",
    "            \"average\": avg_similarity,\n",
    "            \"best_pair\": best_pair,\n",
    "            \"worst_pair\": worst_pair,\n",
    "        }\n",
    "        \n",
    "        # Check if this bucket has the most trajectories\n",
    "        if len(trajectories) > max_bucket_size:\n",
    "            max_bucket_size = len(trajectories)\n",
    "            max_bucket = bucket\n",
    "    else:\n",
    "        # Handle missing trajectory pairs in the CSV\n",
    "        bucket_stats[bucket] = {\"best\": None, \"worst\": None, \"average\": None}\n",
    "\n",
    "# Display results for the bucket with the most trajectories\n",
    "if max_bucket:\n",
    "    print(f\"Bucket with the most trajectories: {max_bucket} ({max_bucket_size} trajectories)\")\n",
    "    best_similarity, best_t1, best_t2 = bucket_stats[max_bucket][\"best_pair\"]\n",
    "    worst_similarity, worst_t1, worst_t2 = bucket_stats[max_bucket][\"worst_pair\"]\n",
    "    print(f\"Most Similar Pair (Lowest similarity value): {best_similarity} between {best_t1} and {best_t2}\")\n",
    "    print(f\"Least Similar Pair (Highest similarity value): {worst_similarity} between {worst_t1} and {worst_t2}\")\n",
    "    print(f\"Average Similarity inside bucket '{max_bucket}': {bucket_stats[max_bucket]['average']:.2f}\")\n",
    "else:\n",
    "    print(\"No buckets with more than one trajectory found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1406260090323557"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similarity_between_files(file1, file2, similarity_df):\n",
    "    # Clean the filenames by removing the '.txt' extension\n",
    "    file1_clean = file1.replace('.txt', '')\n",
    "    file2_clean = file2.replace('.txt', '')\n",
    "\n",
    "    # Ensure correct row-column order (file1_clean should be lexicographically larger)\n",
    "    if file1_clean < file2_clean:\n",
    "        file1_clean, file2_clean = file2_clean, file1_clean  # Swap for correct matrix access\n",
    "\n",
    "    # Check if both files are in the DataFrame\n",
    "    if file1_clean in similarity_df.index and file2_clean in similarity_df.columns:\n",
    "        similarity_value = float(similarity_df.at[file1_clean, file2_clean])  # Get the similarity value\n",
    "        return similarity_value\n",
    "    else:\n",
    "        # If files are not in the matrix, return None\n",
    "        return None\n",
    "\n",
    "\n",
    "get_similarity_between_files(\"R_DUB.txt\", \"R_ECP.txt\", similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CityHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_CAV.txt AO_AH_AC\n",
      "R_CAV.txt \n",
      "R_CAV.txt \n",
      "R_CAV.txt AK_AL_AO\n",
      "R_DYX.txt \n",
      "R_DYX.txt AC_AC_AA_AA\n",
      "R_DYX.txt AD_AD_AD_AD_AR\n",
      "R_DYX.txt \n",
      "R_CDU.txt AG_AB\n",
      "R_CDU.txt AC_AC_AC_AC_AM_AB\n",
      "R_CDU.txt AD_AD_AL\n",
      "R_CDU.txt \n",
      "R_ECN.txt AN_AN\n",
      "R_ECN.txt AN\n",
      "R_ECN.txt AG_AF_AK_AD_AR\n",
      "R_ECN.txt \n",
      "R_EFS.txt \n",
      "R_EFS.txt AC_AH\n",
      "R_EFS.txt AD_AR\n",
      "R_EFS.txt AJ_AB\n",
      "R_DOY.txt AG_AJ\n",
      "R_DOY.txt AH_AC_AA_AA\n",
      "R_DOY.txt AD_AI\n",
      "R_DOY.txt AR\n",
      "R_CFV.txt \n",
      "R_CFV.txt \n",
      "R_CFV.txt AD_AA_AD_AF\n",
      "R_CFV.txt AJ\n",
      "R_AVK.txt AG_AB_AB_AH_AC\n",
      "R_AVK.txt AA_AC_AH\n",
      "R_AVK.txt \n",
      "R_AVK.txt AL\n",
      "R_EDS.txt AH_AC_AD_AJ_AG\n",
      "R_EDS.txt AI_AA\n",
      "R_EDS.txt \n",
      "R_EDS.txt AC_AL_AO_AR_AR\n",
      "R_COQ.txt AB_AG\n",
      "R_COQ.txt AC_AC_AH_AA\n",
      "R_COQ.txt AI\n",
      "R_COQ.txt AR\n",
      "R_AKY.txt AG\n",
      "R_AKY.txt AA\n",
      "R_AKY.txt AR_AD_AR\n",
      "R_AKY.txt \n",
      "R_BCU.txt \n",
      "R_BCU.txt AA\n",
      "R_BCU.txt AF_AD_AR_AR\n",
      "R_BCU.txt AJ\n",
      "R_CCZ.txt AE\n",
      "R_CCZ.txt \n",
      "R_CCZ.txt AF_AD_AR\n",
      "R_CCZ.txt AJ\n",
      "R_CPD.txt AD_AJ_AB_AG_AD_AJ_AB\n",
      "R_CPD.txt AH_AM\n",
      "R_CPD.txt \n",
      "R_CPD.txt AL_AO_AR_AO_AL_AK\n",
      "R_AVF.txt AB_AG\n",
      "R_AVF.txt AC_AC_AA\n",
      "R_AVF.txt AD_AI\n",
      "R_AVF.txt AR_AQ\n",
      "R_AVD.txt AB\n",
      "R_AVD.txt AC_AC_AH\n",
      "R_AVD.txt AD_AD_AD\n",
      "R_AVD.txt \n",
      "R_CYW.txt AN_AB\n",
      "R_CYW.txt AC_AC_AC_AC\n",
      "R_CYW.txt AD_AK_AG_AD_AD\n",
      "R_CYW.txt \n",
      "R_AVB.txt AG_AB\n",
      "R_AVB.txt AA_AC_AC\n",
      "R_AVB.txt AI_AD_AD\n",
      "R_AVB.txt AF_AQ_AR\n",
      "R_BNG.txt AO_AF\n",
      "R_BNG.txt \n",
      "R_BNG.txt AL\n",
      "R_BNG.txt AS_AG\n",
      "R_DVK.txt AO_AH_AC_AD_AJ_AG\n",
      "R_DVK.txt AA\n",
      "R_DVK.txt \n",
      "R_DVK.txt AL_AO_AR\n",
      "R_ADV.txt AF_AB\n",
      "R_ADV.txt AH\n",
      "R_ADV.txt \n",
      "R_ADV.txt AS_AS_AK\n",
      "R_EHK.txt AB_AN\n",
      "R_EHK.txt AH_AC_AC_AN\n",
      "R_EHK.txt AD_AD_AK_AG\n",
      "R_EHK.txt \n",
      "R_AKK.txt AR_AR_AQ_AG\n",
      "R_AKK.txt AA_AC\n",
      "R_AKK.txt AI_AD_AD\n",
      "R_AKK.txt AR\n",
      "R_BRF.txt AQ_AR\n",
      "R_BRF.txt AQ_AA\n",
      "R_BRF.txt AR_AR\n",
      "R_BRF.txt \n",
      "R_ARU.txt AN_AN\n",
      "R_ARU.txt AD_AN_AC_AC\n",
      "R_ARU.txt AG_AG_AK_AD_AD_AR\n",
      "R_ARU.txt \n",
      "R_DUB.txt AD_AJ_AG_AN\n",
      "R_DUB.txt AA\n",
      "R_DUB.txt AR_AD_AK\n",
      "R_DUB.txt AR\n",
      "R_CEX.txt AQ_AR_AR_AR_AQ_AG\n",
      "R_CEX.txt AA_AC\n",
      "R_CEX.txt \n",
      "R_CEX.txt AR\n",
      "R_DJT.txt AH_AC_AD_AJ_AB\n",
      "R_DJT.txt AH_AC\n",
      "R_DJT.txt \n",
      "R_DJT.txt AL_AO\n",
      "R_EBK.txt \n",
      "R_EBK.txt AS_AL\n",
      "R_EBK.txt AO_AS_AB\n",
      "R_EBK.txt AA_AN_AB_AJ\n",
      "R_CCQ.txt AO_AH_AC\n",
      "R_CCQ.txt AP\n",
      "R_CCQ.txt \n",
      "R_CCQ.txt AE_AQ\n",
      "R_AWU.txt \n",
      "R_AWU.txt AC_AH_AM\n",
      "R_AWU.txt AD\n",
      "R_AWU.txt AK\n",
      "R_DGV.txt AG_AD_AJ_AH_AC_AH_AH\n",
      "R_DGV.txt \n",
      "R_DGV.txt \n",
      "R_DGV.txt AR_AO_AL\n",
      "R_BTH.txt AB_AO\n",
      "R_BTH.txt AH\n",
      "R_BTH.txt \n",
      "R_BTH.txt \n",
      "R_DUV.txt \n",
      "R_DUV.txt AL\n",
      "R_DUV.txt AF_AS_AB_AO\n",
      "R_DUV.txt AJ_AB_AN_AN_AA\n",
      "R_AFZ.txt AB_AH\n",
      "R_AFZ.txt AC_AH\n",
      "R_AFZ.txt \n",
      "R_AFZ.txt AL\n",
      "R_EDX.txt AD_AJ_AG\n",
      "R_EDX.txt AP_AA\n",
      "R_EDX.txt AD\n",
      "R_EDX.txt AE_AQ_AR\n",
      "R_CIV.txt AG\n",
      "R_CIV.txt AC_AA\n",
      "R_CIV.txt AD\n",
      "R_CIV.txt AR\n",
      "R_ABU.txt AN_AG\n",
      "R_ABU.txt AN_AC\n",
      "R_ABU.txt AG_AK_AD\n",
      "R_ABU.txt AR\n",
      "R_BDC.txt AF_AF_AO_AH_AC\n",
      "R_BDC.txt AK\n",
      "R_BDC.txt \n",
      "R_BDC.txt \n",
      "R_BML.txt AB_AL\n",
      "R_BML.txt AH_AM_AB\n",
      "R_BML.txt AL\n",
      "R_BML.txt AL_AO_AK_AD\n",
      "R_ECP.txt AD_AJ_AG\n",
      "R_ECP.txt AA\n",
      "R_ECP.txt AR_AD\n",
      "R_ECP.txt AL_AO_AR\n",
      "R_AZS.txt AR_AR_AQ\n",
      "R_AZS.txt AS_AF_AQ\n",
      "R_AZS.txt AA_AN\n",
      "R_AZS.txt AA\n",
      "R_BFS.txt AH_AC\n",
      "R_BFS.txt AI\n",
      "R_BFS.txt AM_AM\n",
      "R_BFS.txt AE_AF_AE_AQ_AR\n",
      "R_BUX.txt AB_AB_AD_AJ_AJ\n",
      "R_BUX.txt AH_AH_AC\n",
      "R_BUX.txt \n",
      "R_BUX.txt AL_AO\n",
      "R_CCJ.txt AG_AD_AJ_AH_AC_AO\n",
      "R_CCJ.txt AA\n",
      "R_CCJ.txt \n",
      "R_CCJ.txt AR_AO_AL\n",
      "R_CRC.txt AG_AD_AJ_AG_AB\n",
      "R_CRC.txt AA_AA_AC_AC\n",
      "R_CRC.txt AD_AD\n",
      "R_CRC.txt AR_AQ_AQ\n",
      "R_ARC.txt AQ_AQ_AG\n",
      "R_ARC.txt AA_AA_AC\n",
      "R_ARC.txt AI_AI\n",
      "R_ARC.txt AR\n",
      "R_DDN.txt AG_AJ_AD\n",
      "R_DDN.txt AA_AA\n",
      "R_DDN.txt AD_AR\n",
      "R_DDN.txt AR_AO_AL\n",
      "R_DAQ.txt \n",
      "R_DAQ.txt \n",
      "R_DAQ.txt AA_AD_AR_AR\n",
      "R_DAQ.txt \n",
      "R_CNH.txt \n",
      "R_CNH.txt AM_AC\n",
      "R_CNH.txt \n",
      "R_CNH.txt AP\n",
      "Bucket 316124876320608819949206964099894625006: ['R_CAV.txt', 'R_CCQ.txt']\n",
      "Bucket 82332263323914296566372529678324145705: ['R_CAV.txt', 'R_CAV.txt', 'R_DYX.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_CFV.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_BDC.txt', 'R_BDC.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_CNH.txt', 'R_CNH.txt']\n",
      "Bucket 82332263323914296566372529678324145705: ['R_CAV.txt', 'R_CAV.txt', 'R_DYX.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_CFV.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_BDC.txt', 'R_BDC.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_CNH.txt', 'R_CNH.txt']\n",
      "Bucket 19673639622178151168245716671632269238: ['R_CAV.txt']\n",
      "Largest Bucket: 82332263323914296566372529678324145705\n",
      "Total Buckets: 117\n",
      "Buckets with more than one trajectory: 25\n",
      "Buckets with only one trajectory: 92\n",
      "Largest Bucket Size: 43\n",
      "Percentage of buckets with more than one trajectory: 21.37%\n",
      "Percentage of buckets with only one trajectory: 78.63%\n"
     ]
    }
   ],
   "source": [
    "import cityhash\n",
    "import os\n",
    "from constants import NUMBER_OF_TRAJECTORIES\n",
    "\n",
    "# Paths\n",
    "ROME_HASHED_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/hashed_data/disk/rome/\"\n",
    "ROME_HASHED_TRAJECTORIES_FOLDER_META_FILE = f\"{ROME_HASHED_TRAJECTORIES_OUTPUT_FOLDER}META-{NUMBER_OF_TRAJECTORIES}.txt\"\n",
    "\n",
    "ROME_FULL_TRAJECTORIES_OUTPUT_FOLDER = \"../../dataset/rome/output/\"\n",
    "\n",
    "# Dictionary for the bucket system\n",
    "bucket_system = {}\n",
    "\n",
    "# Get filenames from the metafile\n",
    "files = mfh.read_meta_file(ROME_HASHED_TRAJECTORIES_FOLDER_META_FILE)\n",
    "\n",
    "ABU_HASH = []\n",
    "\n",
    "# Iterate through trajectory files and read their hashes\n",
    "for filename in files:\n",
    "    file_path = os.path.join(ROME_HASHED_TRAJECTORIES_OUTPUT_FOLDER, filename)\n",
    "    \n",
    "    # Read the hashes for the trajectory\n",
    "    trajectory_hashes = fh.read_hash_file(file_path)\n",
    "    \n",
    "    # Iterate over each layer's hash\n",
    "    \n",
    "    for layer_hash in trajectory_hashes:\n",
    "\n",
    "        # Convert the list of coordinates into a string\n",
    "        hash_string = \"_\".join(map(str, layer_hash))\n",
    "        print(filename, hash_string)\n",
    "        # Use CityHash for creating a unique key\n",
    "        hash_key = cityhash.CityHash128(hash_string)\n",
    "        if hash_key == \"\":\n",
    "            print(filename, hash_string)\n",
    "        \n",
    "        # Place trajectory into the appropriate bucket\n",
    "        if hash_key not in bucket_system:\n",
    "            bucket_system[hash_key] = []\n",
    "        bucket_system[hash_key].append(filename)\n",
    "        if filename == \"R_CAV.txt\":\n",
    "            ABU_HASH.append(hash_key)\n",
    "\n",
    "\n",
    "# Print the contents of the buckets for \"R_ABU.txt\"\n",
    "for hash_key in ABU_HASH:\n",
    "    print(f\"Bucket {hash_key}: {bucket_system[hash_key]}\")\n",
    "   \n",
    "\n",
    "\n",
    "# Analyze and display results\n",
    "\n",
    "total_buckets = len(bucket_system)\n",
    "buckets_with_multiple = sum(1 for trajectories in bucket_system.values() if len(trajectories) > 1)\n",
    "buckets_with_single = total_buckets - buckets_with_multiple\n",
    "largest_bucket_size = max(len(trajectories) for trajectories in bucket_system.values())\n",
    "largest_bucket = max(bucket_system, key=lambda key: len(bucket_system[key]))\n",
    "print(f\"Largest Bucket: {largest_bucket}\")\n",
    "\n",
    "print(f\"Total Buckets: {total_buckets}\")\n",
    "print(f\"Buckets with more than one trajectory: {buckets_with_multiple}\")\n",
    "print(f\"Buckets with only one trajectory: {buckets_with_single}\")\n",
    "print(f\"Largest Bucket Size: {largest_bucket_size}\")\n",
    "\n",
    "# Optional: Display distribution percentages\n",
    "multiple_bucket_percentage = (buckets_with_multiple / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "single_bucket_percentage = (buckets_with_single / total_buckets) * 100 if total_buckets > 0 else 0\n",
    "\n",
    "print(f\"Percentage of buckets with more than one trajectory: {multiple_bucket_percentage:.2f}%\")\n",
    "print(f\"Percentage of buckets with only one trajectory: {single_bucket_percentage:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABU.txt buckets: ['R_CAV.txt', 'R_CCQ.txt']\n",
      "ABU.txt buckets: ['R_CAV.txt', 'R_CAV.txt', 'R_DYX.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_CFV.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_BDC.txt', 'R_BDC.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_CNH.txt', 'R_CNH.txt']\n",
      "ABU.txt buckets: ['R_CAV.txt', 'R_CAV.txt', 'R_DYX.txt', 'R_DYX.txt', 'R_CDU.txt', 'R_ECN.txt', 'R_EFS.txt', 'R_CFV.txt', 'R_CFV.txt', 'R_AVK.txt', 'R_EDS.txt', 'R_AKY.txt', 'R_BCU.txt', 'R_CCZ.txt', 'R_CPD.txt', 'R_AVD.txt', 'R_CYW.txt', 'R_BNG.txt', 'R_DVK.txt', 'R_ADV.txt', 'R_EHK.txt', 'R_BRF.txt', 'R_ARU.txt', 'R_CEX.txt', 'R_DJT.txt', 'R_EBK.txt', 'R_CCQ.txt', 'R_AWU.txt', 'R_DGV.txt', 'R_DGV.txt', 'R_BTH.txt', 'R_BTH.txt', 'R_DUV.txt', 'R_AFZ.txt', 'R_BDC.txt', 'R_BDC.txt', 'R_BUX.txt', 'R_CCJ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_DAQ.txt', 'R_CNH.txt', 'R_CNH.txt']\n",
      "ABU.txt buckets: ['R_CAV.txt']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ABU_HASH)):\n",
    "    print(f\"ABU.txt buckets: {bucket_system[ABU_HASH[i]]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket with the most trajectories: 82332263323914296566372529678324145705 (43 trajectories)\n",
      "Most Similar Pair (Lowest similarity value): 0.0 between R_CAV.txt and R_CAV.txt\n",
      "Least Similar Pair (Highest similarity value): 10.327691364230498 between R_AVD.txt and R_BDC.txt\n",
      "Average Similarity inside bucket '82332263323914296566372529678324145705': 2.55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "ROME_TRUE_SIMILARITY_FILE = \"../../results_true/similarity_values/rome/dtw/rome-dtw-3050.csv\"\n",
    "\n",
    "# Load the similarity matrix CSV\n",
    "similarity_df = pd.read_csv(ROME_TRUE_SIMILARITY_FILE, index_col=0)\n",
    "\n",
    "# Results storage for statistics\n",
    "bucket_stats = {}\n",
    "\n",
    "# Filter buckets to process only those with more than one trajectory\n",
    "filtered_buckets = {bucket: trajectories for bucket, trajectories in bucket_system.items() if len(trajectories) > 1}\n",
    "\n",
    "# Track the bucket with the most trajectories\n",
    "max_bucket_size = 0\n",
    "max_bucket = None\n",
    "\n",
    "# Process each filtered bucket\n",
    "for bucket, trajectories in filtered_buckets.items():\n",
    "    similarities = []\n",
    "    best_pair = None\n",
    "    worst_pair = None\n",
    "\n",
    "    # Compute all pairwise similarities within the bucket\n",
    "    for t1, t2 in combinations(trajectories, 2):\n",
    "        # Strip `.txt` from the trajectory names\n",
    "        t1_clean = t1.replace('.txt', '')\n",
    "        t2_clean = t2.replace('.txt', '')\n",
    "\n",
    "        # Ensure correct row-column order (t1_clean should be lexicographically larger)\n",
    "        if t1_clean < t2_clean:\n",
    "            t1_clean, t2_clean = t2_clean, t1_clean  # Swap for correct matrix access\n",
    "\n",
    "        if t1_clean in similarity_df.index and t2_clean in similarity_df.columns:\n",
    "            similarity = float(similarity_df.at[t1_clean, t2_clean])  # Convert to native Python float\n",
    "            similarities.append(similarity)\n",
    "            \n",
    "            # Track the pair with the best similarity (lowest value)\n",
    "            if best_pair is None or similarity < best_pair[0]:\n",
    "                best_pair = (similarity, t1, t2)\n",
    "\n",
    "            # Track the pair with the worst similarity (highest value)\n",
    "            if worst_pair is None or similarity > worst_pair[0]:\n",
    "                worst_pair = (similarity, t1, t2)\n",
    "\n",
    "    if similarities:\n",
    "        best_similarity = min(similarities)\n",
    "        worst_similarity = max(similarities)\n",
    "        avg_similarity = sum(similarities) / len(similarities)\n",
    "        bucket_stats[bucket] = {\n",
    "            \"best\": best_similarity,\n",
    "            \"worst\": worst_similarity,\n",
    "            \"average\": avg_similarity,\n",
    "            \"best_pair\": best_pair,\n",
    "            \"worst_pair\": worst_pair,\n",
    "        }\n",
    "        \n",
    "        # Check if this bucket has the most trajectories\n",
    "        if len(trajectories) > max_bucket_size:\n",
    "            max_bucket_size = len(trajectories)\n",
    "            max_bucket = bucket\n",
    "    else:\n",
    "        # Handle missing trajectory pairs in the CSV\n",
    "        bucket_stats[bucket] = {\"best\": None, \"worst\": None, \"average\": None}\n",
    "\n",
    "# Display results for the bucket with the most trajectories\n",
    "if max_bucket:\n",
    "    print(f\"Bucket with the most trajectories: {max_bucket} ({max_bucket_size} trajectories)\")\n",
    "    best_similarity, best_t1, best_t2 = bucket_stats[max_bucket][\"best_pair\"]\n",
    "    worst_similarity, worst_t1, worst_t2 = bucket_stats[max_bucket][\"worst_pair\"]\n",
    "    print(f\"Most Similar Pair (Lowest similarity value): {best_similarity} between {best_t1} and {best_t2}\")\n",
    "    print(f\"Least Similar Pair (Highest similarity value): {worst_similarity} between {worst_t1} and {worst_t2}\")\n",
    "    print(f\"Average Similarity inside bucket '{max_bucket}': {bucket_stats[max_bucket]['average']:.2f}\")\n",
    "else:\n",
    "    print(\"No buckets with more than one trajectory found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
